{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0cfd58",
   "metadata": {},
   "source": [
    "# Tomato Classifier\n",
    "---\n",
    "This file will detail how you can use this repo, as well as how to reproduce the results from the paper:\n",
    "\n",
    "# **This file is runnable, if you want to re-evaluate to see that our trained models do indeed produce the results shown.**\n",
    "\n",
    "### To understand each file, this is how you read the config\n",
    "\n",
    "\t dataset_dir :  Tomato-Merged # The current dataset being used. Tomato-Merged is our unioned dataset, PlantVillage-Tomato is just PlantVillage\n",
    "\t checkpoint_filepath :  MobileVITNoCLAHE # The model Name\n",
    "\t add_dense :  True \n",
    "\t img_height :  224\n",
    "\t img_width :  224\n",
    "\t batch_size :  16\n",
    "\t epochs :  100\n",
    "\t n_classes :  10\n",
    "\t seed :  42\n",
    "\t fig_format :  .png\n",
    "\t data_augmentations\n",
    "\t\t TRAIN_AUG :  True # If this is true Augmentation is ON\n",
    "\t\t VALID_AUG :  False\n",
    "\t\t TEST_AUG :  False\n",
    "\t\t isVIT :  True # This is only true if you are running the MobileVIT vision transformer. Changes images from 224x224 to 256x256\n",
    "\t\t applyCLAHE :  False # If this is true, CLAHE pre-processing is ON\n",
    "\t\t rotation_range :  20\n",
    "\t\t horizontal_flip :  True\n",
    "\t\t width_shift_range :  0.2\n",
    "\t\t height_shift_range :  0.2\n",
    "\t\t shear_range :  0.2\n",
    "\t model_configuration\n",
    "\t\t backbone_name :  mobilevitsmall # Name of the model architecture we are using\n",
    "\t\t optimizer :  adam\n",
    "\t learning_rates\n",
    "\t\t initial_lr :  1e-05\n",
    "\t\t max_lr :  0.0001\n",
    "\t version :  1.0.0\n",
    "\n",
    "Everything that is commented is the only parameters we changed to ensure our parent paper could remain the control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import runEval\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t dataset_dir :  Tomato-Merged\n",
      "\t checkpoint_filepath :  MobileVITCLAHE\n",
      "\t add_dense :  True\n",
      "\t img_height :  224\n",
      "\t img_width :  224\n",
      "\t batch_size :  16\n",
      "\t epochs :  100\n",
      "\t n_classes :  10\n",
      "\t seed :  42\n",
      "\t fig_format :  .png\n",
      "\t data_augmentations\n",
      "\t\t TRAIN_AUG :  True\n",
      "\t\t VALID_AUG :  False\n",
      "\t\t TEST_AUG :  False\n",
      "\t\t isVIT :  True\n",
      "\t\t applyCLAHE :  True\n",
      "\t\t rotation_range :  20\n",
      "\t\t horizontal_flip :  True\n",
      "\t\t width_shift_range :  0.2\n",
      "\t\t height_shift_range :  0.2\n",
      "\t\t shear_range :  0.2\n",
      "\t model_configuration\n",
      "\t\t backbone_name :  mobilevitsmall\n",
      "\t\t optimizer :  adam\n",
      "\t learning_rates\n",
      "\t\t initial_lr :  1e-05\n",
      "\t\t max_lr :  0.0001\n",
      "\t version :  1.0.0\n",
      "[INFO] Augmentation is applied on training data generator\n",
      "Found 12209 validated image filenames belonging to 10 classes.\n",
      "[INFO] No Augmentation is applied on validation data generator\n",
      "Found 4070 validated image filenames belonging to 10 classes.\n",
      "[INFO] No Augmentation is applied on Test data generator\n",
      "Found 4070 validated image filenames belonging to 10 classes.\n",
      "[INFO] Total Number of Test instances: 4080\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobileViT_Small_FeatureExtr  (None, 8, 8, 640)        4949888   \n",
      " actor (KerasLayer)                                              \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8, 8, 640)        2560      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8, 8, 128)         82048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8, 8, 64)          8256      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,083,978\n",
      "Trainable params: 5,070,314\n",
      "Non-trainable params: 13,664\n",
      "_________________________________________________________________\n",
      "[INFO] Generating predictions...\n",
      " 12/255 [>.............................] - ETA: 33s"
     ]
    }
   ],
   "source": [
    "runEval(\"VITCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f46bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"MobileVITCLAHE/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type MobileVITCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"VITNoCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135902dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"MobileVITNoCLAHE/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd38a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type MobileVITNoCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"VITOnlyCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"MobileVITCLAHENoAug/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type MobileVITClaheNoAug\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d51745",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"VITDense.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"MobileVITDense/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff740b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type MobileVITDense\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee54f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3LargeCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3LargeCLAHE/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3LargeCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5261ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3LargeNoCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3LargeNoCLAHE/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3LargeNoCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3LargeOnlyCLAHE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3LargeCLAHENoAug/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3LargeCLAHENoAug\\classification_report.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3LargeDense.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce493dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3LargeDense/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3LargeDense\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed43ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3SmallCLAHE.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3SmallCLAHE/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95812164",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3SmallCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b831813",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3SmallNoCLAHE.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3SmallNoCLAHE/graphs/roc_curves.png\",height=1000,width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6427a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3SmallNoCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411eda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3SmallOnlyCLAHE.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3SmallOnlyCLAHE/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3SmallOnlyCLAHE\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V3SmallDense.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V3SmallDense/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V3SmallDense\\classification_report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEval(\"V2ParentPaperBest.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"V2ParentPaperOnNewData/graphs/roc_curves.png\",height=1000,width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27551679",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type V2ParentPaperOnNewData\\classification_report.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
